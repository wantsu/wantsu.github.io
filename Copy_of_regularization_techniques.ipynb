{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of regularization_techniques.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xehWl8O5Yd5I"
      },
      "source": [
        "### Regularizaton techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4UJ884i_hdn"
      },
      "source": [
        "### Introduction\r\n",
        "Regularization is widely used to prevent Neural Networks from overfitting, such as weight decay, dropout, early stop etc.  In Computer Vision, image augmentation techniques like CutOut, Mixup and CutMix can also act as a regularizer and  is crucial to improve model's generalization ability. This nodebook provides a collection of resources (papers or codes) about these common regularization methods, aiming to help you to understand the core implementation and  to make it easy for you to adopt it in your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8cMmav-_8k9"
      },
      "source": [
        "### 1.Cutout, [code](https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py) \r\n",
        "\\begin{align*} M_i &= \\text{random}(h, w)\\\\ x &= x \\odot M_i \\end{align*}\r\n",
        "where $M_i$ is a mask and $i$ is the index of holes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eQZeCDqAuMf"
      },
      "source": [
        "### 2.Mixup\r\n",
        "\\begin{align*} x &= \\lambda x_i + (1 - \\lambda)x_j, \\qquad \\text{where } x_i, x_j \\text{ are raw input vectors}\\\\ y &= \\lambda y_i + (1 - \\lambda)y_j \\qquad \\text{where } y_i, y_j \\text{ are raw input vectors} \\end{align*}\r\n",
        "\r\n",
        "where $\\lambda$ is the mixing rate.\r\n",
        "\r\n",
        "```python\r\n",
        "class Mix_Up(object):\r\n",
        "  \"\"\"\r\n",
        "  args: alpha, int, beta parameter used to sample lambda from beta distribution\r\n",
        "  p: proba that applied mixing up\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, alpha=1.0, mixing_rate=0.3, p=0.5):\r\n",
        "        super().__init__()\r\n",
        "        self.alpha = alpha\r\n",
        "        self.p = p\r\n",
        "\r\n",
        "  def mix_batch(self, images, labels):\r\n",
        "        # sample mixing rate form bete distribution\r\n",
        "        lam = np.random.beta(self.alpha, self.alpha)\r\n",
        "        batch_size = images.shape[0]\r\n",
        "        indexs = torch.randperm(batch_size)\r\n",
        "\r\n",
        "        # mixing pairs of images\r\n",
        "        images_mixed = images*lam + images[indexs,:]*(1-lam)\r\n",
        "        labels_a, labels_b = labels, labels[indexs] \r\n",
        "        \r\n",
        "        return images_mixed, labels_a, labels_b, lam\r\n",
        "\r\n",
        "   def mix_criterion(self, criterion, pred, labels_a, labels_b, lam):\r\n",
        "        return criterion(pred, labels_a)*lam + criterion(pred, labels_b)*(1-lam)\r\n",
        "    \r\n",
        "   def apply_mix(self,):\r\n",
        "        return True if np.random.random() < self.p else False    \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZpMC_FyBAiV"
      },
      "source": [
        "### 3.CutMix\r\n",
        "\r\n",
        "\\begin{align*} x &= M_i \\odot x_i + (1 - M_i) \\odot x_j, \\qquad \\text{where } x_i, x_j \\text{ are raw input vectors}\\\\ y &= \\lambda y_i + (1 - \\lambda)y_j \\qquad\\qquad\\qquad \\text{where } y_i, y_j \\text{ are raw input vectors} \\end{align*}\r\n",
        "\r\n",
        "where $M_i$ is a mask, $\\lambda$ is the mix rate.\r\n",
        "\r\n",
        "``` python\r\n",
        "  class Cut_Mix(object):\r\n",
        "    def __init__(self, alpha=1.0, p=0.5):\r\n",
        "      super().__init__()\r\n",
        "      self.alpha = alpha\r\n",
        "      self.p = p\r\n",
        "    \r\n",
        "    def mix_batch(self, images, labels):\r\n",
        "        # sample mixing rate form bete distribution\r\n",
        "        lam = np.random.beta(self.alpha, self.alpha)\r\n",
        "        batch_size = images.shape[0]\r\n",
        "        indexs = torch.randperm(batch_size)\r\n",
        "\r\n",
        "        # mixing pairs of images\r\n",
        "        mask, lam = self.create_mask(images[0], lam)\r\n",
        "        images_mixed = images*mask + images[indexs,:]*(1-mask)\r\n",
        "\r\n",
        "        labels_a, labels_b = labels, labels[indexs] \r\n",
        "        return images_mixed, labels_a, labels_b, lam\r\n",
        "    \r\n",
        "    def create_mask(self, img, lam):\r\n",
        "        C, H, W = img.shape\r\n",
        "\r\n",
        "        mask = np.ones((H, W), np.float32)\r\n",
        "        \r\n",
        "        # cut size\r\n",
        "        cut_rat = np.sqrt(1. - lam)\r\n",
        "        cut_w = np.int(W * cut_rat)\r\n",
        "        cut_h = np.int(H * cut_rat)\r\n",
        "\r\n",
        "        # uniform\r\n",
        "        cx = np.random.randint(W)\r\n",
        "        cy = np.random.randint(H)\r\n",
        "\r\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\r\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\r\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\r\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\r\n",
        "        \r\n",
        "        mask[bby1: bby2, bbx1: bbx2] = 0.\r\n",
        "\r\n",
        "        mask = torch.from_numpy(mask)\r\n",
        "        mask = mask.expand_as(img)\r\n",
        "        lam = 1 - (bbx2 - bbx1) * (bby2 - bby1) / (H * W) # compute the real lambda\r\n",
        "        return mask, lam\r\n",
        "    \r\n",
        "    def mix_criterion(self, criterion, pred, labels_a, labels_b, lam):\r\n",
        "        return criterion(pred, labels_a)*lam + criterion(pred, labels_b)*(1-lam)\r\n",
        "    \r\n",
        "    def apply_mix(self,):\r\n",
        "        return True if np.random.random() < self.p else False\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OYlzg6DVGL5"
      },
      "source": [
        "We can also combine Mxing up and Cut mix together into a more powerful regularization function, for example:\r\n",
        "\r\n",
        "``` python\r\n",
        "class Cut_Mix_Up(object):\r\n",
        "    def __init__(self, alpha=1.0, mixing_rate=0.3, p=0.5):\r\n",
        "          super().__init__()\r\n",
        "          self.alpha = alpha\r\n",
        "          self.mixing_rate = mixing_rate  # mixing up prob\r\n",
        "          self.p = p\r\n",
        "          \r\n",
        "    def mix_batch(self, images, labels):\r\n",
        "      ...\r\n",
        "      # mixing pairs of images\r\n",
        "      if np.random.random() < self.mixing_rate: # apply Mixup\r\n",
        "          images_mixed = images*lam + images[indexs,:]*(1-lam)\r\n",
        "              \r\n",
        "      else: # apply Cutmix\r\n",
        "          mask, lam = self.create_mask(images[0], lam)\r\n",
        "          images_mixed = images*mask + images[indexs,:]*(1-mask)\r\n",
        "      ...\r\n",
        "\r\n",
        "    def create_mask(self, img, lam):\r\n",
        "      ...\r\n",
        "\r\n",
        "    def mix_criterion(self, criterion, pred, labels_a, labels_b, lam):\r\n",
        "        return criterion(pred, labels_a)*lam + criterion(pred, labels_b)*(1-lam)\r\n",
        "\r\n",
        "\r\n",
        "    def apply_mix(self,):\r\n",
        "        return True if np.random.random() < self.p else False\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axLAgcWPBOUV"
      },
      "source": [
        "### 4.Label Smoothing\r\n",
        "\r\n",
        "\\begin{align*}\r\n",
        " CELoss(y, p) &= \\sum^{C}_{k} - y^{LS}_{k} \\log(p_k) = (1 - \\alpha)\\sum^{C}_{k} - y_{k} \\log(p_k) + \\alpha / C \\sum^{C}_{k} - \\log(p_k) \\\\\r\n",
        " y^{LS}_{k} &=  y_k(1 - \\alpha) + \\alpha / C \r\n",
        "\\end{align*}\r\n",
        "\r\n",
        "Where $y_k$ is $1$ if class[k] is ground truth, 0 otherwise. $\\alpha$ is the smoothing rate, $C$ is the number of classes. If we set $\\alpha = 0$, above becomes to standard Cross Entropy loss.\r\n",
        "\r\n",
        "```python\r\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\r\n",
        "    def __init__(self, smoothing=0.1, reduction='mean'):\r\n",
        "        super().__init__()\r\n",
        "        self.reduction = reduction\r\n",
        "        self.alpha = smoothing\r\n",
        "\r\n",
        "    def forward(self, inputs, target):\r\n",
        "        c = inputs.size()[-1]  # num of classes\r\n",
        "        # log_softmax function    \r\n",
        "        probs = F.softmax(inputs, dim=-1)\r\n",
        "        log_preds = torch.log(probs)\r\n",
        "\r\n",
        "        loss = -log_preds.sum(dim=-1)  # reduce class \r\n",
        "        ls_celoss = (1-self.alpha) * F.nll_loss(log_preds, target, reduction=self.reduction) + self.alpha/c*loss\r\n",
        "        return ls_celoss if self.reduction == 'none' else ls_celoss.mean()\r\n",
        "\r\n",
        "```\r\n",
        "Relevant articles:\r\n",
        "> 1. [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)\r\n",
        "> 2. [Label Smoothing](https://leimao.github.io/blog/Label-Smoothing/)\r\n",
        "> 3. [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf), part 7.\r\n",
        "> 4. [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf), part6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnKnfVwlBaMd"
      },
      "source": [
        "### 5.Confidence Penalty (focal loss)\r\n",
        "\r\n",
        "[Focal loss](https://arxiv.org/abs/1708.02002) was first proposed to address the class Imbalanced problem in object detection where background objects are much more than positive objects, hence, author argued that such imbalanced problem is the central factor that limits the detector’s accuracy.\r\n",
        "Instead of directly forcing model to more focus on minority samples, author proposed to modify standard cross-entropy loss that down-weights the loss that assigned to high confidence (well-classified) example with a modulating factor.\r\n",
        "\r\n",
        "Focal Loss is formally defined as \r\n",
        "$FL = \\sum^C_k -y_k[\\alpha_k(1- p_k)^\\gamma \\log(p_k)]$\r\n",
        "\r\n",
        "where $\\alpha$ is a weight factor, $\\gamma$ controls the confidence penalty.\r\n",
        "\r\n",
        "``` python\r\n",
        "class FocalLoss(nn.Module):\r\n",
        "    def __init__(self, alpha=1, gamma=2, reduce=True):\r\n",
        "        super(FocalLoss, self).__init__()\r\n",
        "        self.alpha = alpha\r\n",
        "        self.gamma = gamma\r\n",
        "        self.reduce = reduce\r\n",
        "\r\n",
        "    def forward(self, inputs, targets):\r\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\r\n",
        "        p_k = torch.exp(-ce_loss)  # compute confidence\r\n",
        "        focal_loss = self.alpha * (1 - p_k)**self.gamma * ce_loss\r\n",
        "\r\n",
        "        if self.reduce:\r\n",
        "            focal_loss = torch.mean(focal_loss)\r\n",
        "\r\n",
        "        return focal_loss\r\n",
        "```"
      ]
    }
  ]
}